# Create New Release

_(Orchestrate Multiple Feature Units into a Cohesive Release)_

---

## Purpose

Creates and orchestrates a new Release ‚Äî a collection of Feature Units that ship together with integrated testing, acceptance criteria, and deployment planning.

This command implements the Release workflow from `docs/feature_units/standards/release_workflow.md`.

---

## When to Use

**Explicit Command:** Use this command when you know exactly what you want:
- Start a new release (MVP = v1.0.0, next minor = v1.1.0, etc.)
- Plan multi-FU work with dependency resolution
- Generate execution schedules with parallelization
- Orchestrate FU creation/execution in dependency order
- Run cross-FU integration tests

**Automatic Detection:** This workflow can also be triggered automatically via `.cursor/rules/release_detection.md` when you mention release-related patterns in natural language (e.g., "new release", "release v1.1.0"). Both paths execute the same workflow.

---

## Prerequisites

Before running this command:

- [ ] Release ID chosen (format: `vX.Y.Z`, e.g., `v1.0.0`)
- [ ] Release scope roughly defined (what's in, what's out)
- [ ] FU IDs for included work identified (or to be created)

---

## Command Execution

### Step 1: Load Documentation

**Agent: Load required documents:**

```
- docs/feature_units/standards/release_workflow.md (primary workflow)
- docs/feature_units/standards/creating_feature_units.md (FU creation)
- docs/feature_units/standards/execution_instructions.md (FU execution)
- docs/specs/MVP_FEATURE_UNITS.md (if working on MVP)
```

---

### Step 2: Checkpoint 0 ‚Äî Release Planning

**Agent: Check if Release plan exists:**

- Look for `docs/releases/in_progress/{release_id}/release_plan.md`
- Look for `docs/releases/in_progress/{release_id}/manifest.yaml`

**If Release plan exists:**

- Load and validate
- If complete ‚Üí proceed to Step 3
- If incomplete ‚Üí prompt user to complete

**If Release plan does NOT exist:**

**Agent: Prompt user interactively for Release details:**

1. **Release name and version** (e.g., "MVP", "v1.0.0")
2. **Release goal** (1-2 sentence summary of what this ships)
3. **Target ship date** (date or "when ready")
4. **Priority** (P0 critical / P1 high / P2 normal)
5. **Included Feature Units** (list of FU IDs, e.g., "FU-100, FU-101, FU-102" or "all P0 FUs from MVP_FEATURE_UNITS.md")
6. **Excluded scope** (what's explicitly NOT in this release)
7. **Release-level acceptance criteria:**
   - Product acceptance (core workflows, empty/error states)
   - Technical acceptance (test coverage, performance, integrity)
   - Business acceptance (metrics, KPIs)
8. **Cross-FU integration test requirements** (flows that span multiple FUs)
9. **Deployment strategy** (staging_first / canary / full_rollout)
10. **Rollback plan** (how to revert if issues found)
11. **Post-release monitoring plan** (key metrics, alerts)

**Agent: After user input:**

- Generate complete Release plan (`release_plan.md`)
- Create manifest YAML (`manifest.yaml`)
- Create integration test spec (`integration_tests.md`)
- Create status tracker (`status.md`)
- Save all to `docs/releases/in_progress/{release_id}/`

---

### Step 3: Dependency Analysis and Schedule Generation

**Agent: Analyze dependencies:**

1. **Load all FU manifests** for FUs in Release
2. **Extract dependencies** from each FU's `dependencies.requires` field
3. **Build dependency graph**: FU ‚Üí [list of required FUs]
4. **Detect cycles**:
   - If cycle found ‚Üí **REJECT** with error message
   - List cycle: "FU-A ‚Üí FU-B ‚Üí FU-C ‚Üí FU-A"
5. **Validate dependencies**:
   - If any required FU is ‚è≥ Not Started and not in this Release ‚Üí **REJECT**
   - If any required FU is üî® Partial ‚Üí **WARN** but allow with user confirmation
   - If any required FU is ‚úÖ Complete ‚Üí proceed

**Agent: Generate execution schedule:**

1. **Perform topological sort** of FU dependency graph
2. **Group into batches**:
   - Batch 0: FUs with no dependencies
   - Batch 1: FUs that depend only on Batch 0
   - Batch 2: FUs that depend only on Batch 0 or 1
   - ... continue until all FUs assigned to batches
3. **Identify parallelization opportunities**:
   - Within each batch, FUs can execute in parallel
4. **Estimate timeline**:
   - Sum FU effort estimates per batch
   - Account for parallelization (divide by number of parallel agents if applicable)
5. **Save execution schedule** to `execution_schedule.md`

**Agent: Pre-Mortem Analysis:**

1. Identify top 3-5 most likely failure modes for this Release
2. For each failure mode, specify:
   - Early warning signals (metrics, test failures, timeline slips)
   - Mitigating FUs or actions
   - Rollback plan
3. Present pre-mortem to user and ask:
   - "Do these failure modes match your concerns? (yes/no)"
   - "What other failure modes should we plan for?"
4. Incorporate feedback and add "Pre-Mortem" section to Release plan

**Agent: WIP and Parallelization Limits:**

1. Encode limits in `manifest.yaml`:
   - `max_parallel_fus: <number>` (default: 3)
   - `max_high_risk_in_parallel: <number>` (default: 1)
2. Present limits to user: "Proposed limits: max_parallel_fus=3, max_high_risk_in_parallel=1. Approve? (yes/modify)"

**Agent: Machine-Checkable Exit Criteria:**

1. For each Release acceptance criterion, define:
   - Concrete test suite or script that validates it
   - Single metric or query that proves it
2. Add these to `integration_tests.md` with explicit pass/fail conditions
3. Present to user: "Each acceptance criterion now has a machine-checkable test. Review? (yes/modify)"

**Agent: Present schedule to user:**

- Display batch breakdown
- Show which FUs run in parallel
- Show estimated timeline
- Show WIP limits and pre-mortem failure modes
- **STOP and prompt:** "Approve execution schedule? (yes/no/modify)"

**If user approves:**

- Initialize `status.md` with "Decision Log" section (empty initially)
- Update Release status to `in_progress`
- Proceed to Step 4

**If user requests modifications:**

- Adjust schedule based on user input
- Re-present for approval

---

### Step 4: Execute FU Batches (Autonomous)

**Agent: For each batch in execution schedule (in order):**

**Batch Execution Loop:**

1. **Start all FUs in batch:**

   - For each FU in batch:
     - Check if FU spec exists
     - If not, run `Create New Feature Unit` command (Checkpoint 0)
     - If UI FU and no prototype, run `Create Prototype` command
     - If UI FU and prototype not approved, run Checkpoint 1 (Prototype Review)
     - Run `Run Feature Workflow` command to implement FU
     - Run `Final Review` command (Checkpoint 2) for FU approval
   - If multiple FUs in batch, suggest running in parallel (separate agent sessions or worktrees)

2. **Wait for all FUs in batch to complete**

3. **Run cross-FU integration tests:**

   - Execute integration test suite from `integration_tests.md`
   - Test flows that span multiple FUs in this batch
   - If tests fail:
     - **STOP** and report failures to user
     - User decides: fix and retry, skip FU, or abort Release
   - If tests pass, proceed

4. **Update Release status:**
   - Mark batch as `completed` in `status.md`
   - Update progress percentage: `(completed_batches / total_batches) * 100`
   - If any decisions were made during batch execution (scope changes, FU deferrals, etc.), append to Decision Log in `status.md` with timestamp

**After all batches complete:**

- Mark all FUs as `completed` in status tracker
- Proceed to Step 5

---

### Step 5: Checkpoint 1 ‚Äî Mid-Release Review (Optional)

**Agent: Check if mid-release checkpoint is configured:**

- Look for `checkpoint_1_after_batch` in `manifest.yaml`
- If not configured, skip to Step 6

**If configured and batch threshold reached:**

**Agent: Present mid-release status:**

- "Mid-release checkpoint reached"
- "FUs completed: X/Y"
- "Integration tests: [pass/fail summary]"
- "Current batch: N"
- "Remaining batches: [list]"

**STOP and prompt user:**

- "Continue to remaining FUs? (yes/no/pause)"

**If user responds:**

- **yes** ‚Üí continue to remaining batches
- **no** ‚Üí halt execution, mark Release as `paused`
- **pause** ‚Üí halt execution, user can resume later

---

### Step 6: Cross-Release Integration Testing

**Agent: Run full integration test suite:**

1. **Execute all tests** from `integration_tests.md`
2. **Run end-to-end user flows** that span multiple FUs
3. **Verify no regressions** in existing functionality
4. **Run performance benchmarks** (if specified in acceptance criteria)
5. **Verify graph integrity** (0 orphans, 0 cycles if applicable)

**Agent: Check Release-level acceptance criteria:**

- Product acceptance: Core workflows functional?
- Technical acceptance: Test coverage met? Performance targets hit?
- Business acceptance: Metrics instrumented? Analytics ready?

**Agent: Generate integration test report:**

- Save to `integration_test_report.md`
- Include:
  - Pass/fail summary
  - Performance metrics
  - Regression test results
  - Issues found (if any)

**If tests fail:**

- **STOP** and report failures to user
- User decides: fix issues and re-test, or abort Release

**If all tests pass:**

- Proceed to Step 7

---

### Step 7: Checkpoint 2 ‚Äî Pre-Release Sign-Off

**Agent: Present Release summary:**

- FUs completed: X/Y (with list)
- Integration tests: [pass/fail summary with link to report]
- Acceptance criteria: [checklist with ‚úÖ / ‚ùå status]
- Release plan: [link]
- Integration test report: [link]

**STOP and prompt user:**

- "Release {release_id} ready for deployment."
- "All FUs complete: [list with status]"
- "Integration tests passed: [summary]"
- "Acceptance criteria met: [checklist]"
- "Approve for deployment? (yes/no)"
- "Any final changes needed? (list or 'none')"

**If user requests changes:**

- Apply changes
- Re-run affected tests
- Update status
- Repeat Checkpoint 2 until approved

**If user approves:**

- Update Release status to `ready_for_deployment`
- Proceed to Step 8

---

### Step 8: Deployment

**Agent: Execute deployment plan:**

1. **Follow deployment strategy** from Release plan:
   - **staging_first**: Deploy to staging, run smoke tests, then production
   - **canary**: Deploy to small % of users, monitor, then full rollout
   - **full_rollout**: Deploy to all users at once
2. **Run deployment steps:**
   - Build production bundle
   - Run database migrations (if any)
   - Deploy backend services
   - Deploy frontend assets
   - Run health checks
   - Run smoke tests
3. **Verify deployment success:**
   - Health checks pass?
   - Smoke tests pass?
   - Key metrics normal?

**If deployment fails:**

- **STOP** and report failure
- Execute rollback plan
- User investigates and fixes

**If deployment succeeds:**

- Update Release status to `deployed`
- Record deployment timestamp
- Proceed to Step 9

---

### Step 9: Post-Release Monitoring Setup

**Agent: Setup monitoring:**

1. **Verify metrics instrumentation:**
   - Check that key metrics from acceptance criteria are being collected
   - Verify dashboards are accessible
2. **Setup alerts:**
   - Configure alerts for critical metrics (error rates, latency, etc.)
3. **Document monitoring links:**
   - Add links to dashboards, alerts to Release status doc

**Agent: Mark Release as completed:**

- Update status to `completed`
- Move Release files from `in_progress/` to `completed/`:
  ```bash
  mv docs/releases/in_progress/{release_id} docs/releases/completed/{release_id}
  ```

**Agent: Present completion summary:**

- "Release {release_id} deployed and complete!"
- "Deployed at: {timestamp}"
- "Monitoring: {dashboard_links}"
- "FUs shipped: [list]"

---

## Example Interaction

```
User: Create release v1.0.0 for MVP

Agent: Loading Release workflow documentation...

Agent: No existing Release plan found for v1.0.0. Starting interactive Release planning (Checkpoint 0).

Agent: What is the Release name and version?
User: MVP, v1.0.0

Agent: What is the Release goal? (1-2 sentence summary)
User: Ship deterministic Truth Layer with ingestion, extraction, entity resolution, timeline, and MCP access for AI tools. Support multi-user teams.

Agent: What is the target ship date?
User: 2025-03-01

Agent: What is the priority?
User: P0 critical

Agent: Which Feature Units should be included? (list FU IDs or "all P0 from MVP_FEATURE_UNITS")
User: all P0 from MVP_FEATURE_UNITS.md

Agent: [loads MVP_FEATURE_UNITS.md, extracts P0 FUs: FU-100, FU-101, FU-102, FU-103, FU-105, FU-300, FU-700, FU-701]

Agent: What scope is explicitly excluded from this release?
User: Semantic search, Plaid integration, X/Instagram integrations (all post-MVP)

Agent: What are the Release-level acceptance criteria? (product, technical, business)
User: [provides criteria from MVP_EXECUTION_PLAN.md]

Agent: What cross-FU integration tests are required?
User: Full user flow: upload ‚Üí extraction ‚Üí entity resolution ‚Üí event generation ‚Üí timeline display ‚Üí AI query via MCP

Agent: What is the deployment strategy?
User: staging_first

Agent: What is the rollback plan?
User: Revert to previous release tag, restore DB snapshot if needed

Agent: What monitoring plan should be setup post-release?
User: Monitor upload success rate, P95 latency, DAU, orphan node count

Agent: Generating Release plan, manifest, and integration test spec...
Agent: Analyzing FU dependencies...
Agent: Dependency graph:
  FU-100 ‚Üí []
  FU-101 ‚Üí [FU-100]
  FU-102 ‚Üí [FU-100]
  FU-103 ‚Üí [FU-101, FU-102]
  FU-105 ‚Üí [FU-103]
  FU-300 ‚Üí []
  FU-700 ‚Üí []
  FU-701 ‚Üí [FU-700]

Agent: Generating execution schedule...
Agent: Execution schedule:
  Batch 0: FU-100, FU-300, FU-700 (can run in parallel)
  Batch 1: FU-101, FU-102 (can run in parallel, depend on FU-100)
  Batch 2: FU-103 (depends on FU-101, FU-102)
  Batch 3: FU-105, FU-701 (can run in parallel, FU-105 depends on FU-103, FU-701 depends on FU-700)

Agent: Estimated timeline: 11-14 days (with parallelization across 3 agents)

Agent: Approve execution schedule? (yes/no/modify)
User: yes

Agent: Starting batch execution...
Agent: Batch 0: Starting FU-100, FU-300, FU-700 in parallel...
[FU execution proceeds autonomously]
Agent: Batch 0 complete. Running cross-FU integration tests...
Agent: Integration tests passed.
Agent: Batch 1: Starting FU-101, FU-102 in parallel...
[continues through all batches]

Agent: All batches complete. Running full integration test suite...
Agent: All integration tests passed.
Agent: Release v1.0.0 ready for deployment.
Agent: All FUs complete: FU-100 ‚úÖ, FU-101 ‚úÖ, FU-102 ‚úÖ, FU-103 ‚úÖ, FU-105 ‚úÖ, FU-300 ‚úÖ, FU-700 ‚úÖ, FU-701 ‚úÖ
Agent: Integration tests passed: 12/12
Agent: Acceptance criteria met: ‚úÖ Product, ‚úÖ Technical, ‚úÖ Business
Agent: Approve for deployment? (yes/no)
User: yes

Agent: Deploying to staging...
Agent: Staging smoke tests passed.
Agent: Deploying to production...
Agent: Production health checks passed.
Agent: Release v1.0.0 deployed and complete!
Agent: Monitoring: [dashboard links]
```

---

## Agent Instructions

### Load Order

1. `docs/feature_units/standards/release_workflow.md` (primary)
2. `docs/feature_units/standards/creating_feature_units.md` (for FU creation)
3. `docs/feature_units/standards/execution_instructions.md` (for FU execution)
4. `docs/specs/MVP_FEATURE_UNITS.md` (if release_id is v1.0.0 / MVP)

### Constraints

- **NEVER proceed past Checkpoint 0 without complete Release plan**
- **ALWAYS validate FU dependencies before generating schedule**
- **REJECT if circular dependencies detected**
- **ALWAYS run cross-FU integration tests after each batch**
- **ALWAYS get user approval at Checkpoints 0, 1 (if configured), 2**
- **NEVER deploy without passing integration tests and acceptance criteria**

### Forbidden Patterns

- Starting FUs out of dependency order
- Skipping integration tests
- Proceeding without user approval at checkpoints
- Deploying with failing tests or unmet acceptance criteria

---

## References

- **Primary workflow:** `docs/feature_units/standards/release_workflow.md`
- **FU creation:** `docs/feature_units/standards/creating_feature_units.md`
- **FU execution:** `docs/feature_units/standards/execution_instructions.md`
- **MVP inventory:** `docs/specs/MVP_FEATURE_UNITS.md`
