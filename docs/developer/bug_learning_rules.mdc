# Bug Learning and Continuous Improvement Rules

## Purpose

Ensures that when bugs are discovered, agents systematically improve both test coverage and foundation rules to prevent similar bugs in the future.

## Scope

This document covers:
- Required actions when bugs are discovered
- Test coverage improvement requirements
- Foundation rule generalization process
- Learning documentation requirements

This document does NOT cover:
- Bug fix workflow (see `.cursor/rules/bug_fix_detection.mdc`)
- Testing standards (see `docs/testing/testing_standard.md`)
- Rule creation process (see `.cursor/rules/instruction_documentation.mdc`)

## Core Instruction

**ALWAYS increase automated test coverage and integrate learnings into foundation rules when bugs are found.**

## Trigger Patterns

When any bug is discovered through:
- Manual testing
- Production incidents
- User reports
- Integration testing gaps
- Code review findings
- Runtime errors

Agents MUST:
1. Fix the bug
2. Add regression test
3. Analyze why existing tests missed it
4. Strengthen test coverage
5. Evaluate if learnings should be generalized to foundation

## Mandatory Actions When Bugs Are Found

### Step 1: Fix the Bug
Follow standard bug fix workflow (see `.cursor/rules/bug_fix_detection.mdc`).

### Step 2: Add Regression Test
**MUST add test that would have caught the bug.**

**Requirements:**
- Test must fail before fix
- Test must pass after fix
- Test must exercise the specific code path that had the bug
- Test must verify the specific condition that was broken

**Example:**
```typescript
// Bug: Query used .eq("user_id", null) instead of .is("user_id", null)
it("should query fragments with null user_id", async () => {
  // Seed data with null user_id
  await supabase.from("raw_fragments").insert({
    fragment_type: "task",
    fragment_key: "test_field",
    user_id: null,
  });
  
  // This query would have failed before fix
  const { data, error } = await supabase
    .from("raw_fragments")
    .select("*")
    .eq("fragment_type", "task")
    .is("user_id", null); // Correct null handling
  
  expect(error).toBeNull();
  expect(data?.length).toBe(1); // Would have been 0 before fix
});
```

### Step 3: Analyze Test Coverage Gap
**MUST document why existing tests missed the bug.**

**Required analysis:**
1. What test coverage existed?
2. Why did it not catch the bug?
3. What was wrong with the test approach?
4. What test patterns were missing?

**Create analysis document:**
- Location: `docs/reports/{BUG_NAME}_TEST_COVERAGE_GAPS.md`
- Contents: Detailed analysis of why tests missed the bug
- Include: Specific test code that would have caught it
- Include: Root cause analysis (mocking, weak assertions, missing edge cases, etc.)

**Example**: `docs/reports/AUTO_ENHANCEMENT_TEST_COVERAGE_GAPS.md`

### Step 4: Strengthen Test Coverage
**MUST add tests that fill the identified gaps.**

**Required new tests:**
1. **Edge case tests** if bug was in edge case handling
2. **Integration tests without mocks** if bug was hidden by mocks
3. **Strong assertion tests** if weak assertions allowed bug to pass
4. **Foreign key tests** if bug was FK-related
5. **Database state verification tests** if bug left incorrect state
6. **Complete workflow tests** if bug was in multi-step process

**Add to appropriate test file:**
- Unit tests: `tests/unit/` or alongside source
- Integration tests: `tests/integration/`
- E2E tests: `tests/e2e/` or `playwright/`

### Step 5: Evaluate Foundation Generalization
**MUST assess if learnings apply to other repositories.**

**Questions to answer:**
1. Is this bug pattern specific to Neotoma or generic?
2. Would other repositories benefit from this lesson?
3. Are the test improvements generic or specific?
4. Should new rules be added to foundation?

**If 80%+ generic → Generalize to foundation:**
1. Extract generic principles
2. Create or update foundation document
3. Update repository document to reference foundation
4. Keep repository-specific applications separate

**If mostly specific → Keep in repository:**
1. Document in repository rules
2. Reference foundation where applicable
3. No foundation changes needed

**Example**: Auto-enhancement test quality rules were 95% generic, so we:
- Created `foundation/conventions/testing_conventions.md` with generic principles
- Updated `docs/testing/integration_test_quality_rules.mdc` to reference foundation
- Kept Neotoma-specific examples in repository

### Step 6: Document Learning
**MUST create learning summary document.**

**Location**: `docs/reports/{BUG_NAME}_LEARNING_SUMMARY.md`

**Contents:**
- What happened (bug summary)
- Why tests missed it (root cause)
- What we learned (principles)
- Rules created/updated (with links)
- Impact assessment (before/after)
- Validation (how future bugs will be caught)

**Example**: `docs/reports/TEST_QUALITY_LEARNING_SUMMARY.md`

## Workflow

```
Bug Discovered
    ↓
1. Fix bug + add regression test
    ↓
2. Analyze: Why did tests miss it?
    ↓
3. Document gap analysis
    ↓
4. Add tests to fill gaps
    ↓
5. Evaluate: Generic or specific?
    ↓
    ├─→ Generic (80%+)
    │   ├─→ Create/update foundation doc
    │   ├─→ Update repo doc to reference
    │   └─→ Sync rules
    │
    └─→ Specific
        ├─→ Document in repo rules
        └─→ Reference foundation where applicable
    ↓
6. Document learning summary
    ↓
7. Sync to .cursor/rules/
    ↓
Done: Future bugs of this type will be caught
```

## Examples from Practice

### Example 1: Auto-Enhancement Bugs (2026-01-15)

**Bugs found:**
- Foreign key constraint violation
- Observation count query bug
- Fragment query bugs (entity_type column, user_id handling)

**Actions taken:**
1. ✅ Fixed all 4 bugs
2. ✅ Added regression tests (manual verification)
3. ✅ Analyzed coverage gaps → `docs/reports/AUTO_ENHANCEMENT_TEST_COVERAGE_GAPS.md`
4. ✅ Created test quality rules → `docs/testing/integration_test_quality_rules.mdc`
5. ✅ Evaluated generalization → 95% generic
6. ✅ Generalized to foundation → `foundation/conventions/testing_conventions.md`
7. ✅ Documented learning → `docs/reports/TEST_QUALITY_LEARNING_SUMMARY.md`
8. ✅ Synced rules to `.cursor/rules/`

**Impact:**
- Foundation now has reusable testing principles
- Other repos can benefit
- Agents enforce automatically
- Similar bugs will be caught by stronger tests

## Constraints

Agents MUST:
- Add regression test when fixing bugs (never fix without test)
- Analyze why existing tests missed the bug
- Document test coverage gaps
- Add tests to fill identified gaps
- Evaluate if learnings are generic (80%+ → foundation)
- Generalize to foundation when appropriate
- Document learning summary
- Sync rules to `.cursor/rules/`

Agents MUST NOT:
- Fix bugs without adding regression tests
- Skip gap analysis ("tests just missed it")
- Skip test coverage improvements
- Skip foundation generalization evaluation
- Keep generic learnings siloed in repository
- Leave learning undocumented

## Validation Checklist

After discovering and fixing a bug:

- [ ] Bug fixed with code changes
- [ ] Regression test added (fails before fix, passes after)
- [ ] Gap analysis document created (`docs/reports/{BUG_NAME}_TEST_COVERAGE_GAPS.md`)
- [ ] Additional tests added to fill gaps
- [ ] Foundation generalization evaluated (generic vs specific?)
- [ ] Foundation document created/updated (if 80%+ generic)
- [ ] Repository document updated to reference foundation (if generalized)
- [ ] Learning summary documented (`docs/reports/{BUG_NAME}_LEARNING_SUMMARY.md`)
- [ ] Rules synced to `.cursor/rules/` (run `setup_cursor_copies.sh`)
- [ ] Future bugs of this type will be caught (validation)

## Related Documents

- [`.cursor/rules/bug_fix_detection.mdc`](../../.cursor/rules/bug_fix_detection.mdc) — Bug fix workflow
- [`.cursor/rules/instruction_documentation.mdc`](../../.cursor/rules/instruction_documentation.mdc) — Rule creation process
- [`docs/testing/testing_standard.md`](../testing/testing_standard.md) — Testing standards
- [`foundation/conventions/testing_conventions.md`](../../foundation/conventions/testing_conventions.md) — Generic testing principles
